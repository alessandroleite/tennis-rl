{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "This notebook uses the [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/) environment to train two agents to play tennis.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "If the code cell below returns an error, please double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='../env/Tennis_Linux_NoVis/Tennis.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation.  Two continuous actions are available, corresponding to movement toward or away from the net, and jumping. \n",
    "\n",
    "The task is episodic, and in order to solve the environment, the agents must get an average score of +0.5 over 100 consecutive episodes, after taking the maximum over both agents. Specifically,\n",
    "\n",
    "- After each episode, we add up the rewards that each agent received without discounting, to get a score for each agent. This yields two potentially different scores. We then take the maximum of these 2 scores.\n",
    "- This yields a single **score** for each episode.\n",
    "\n",
    "The environment is considered solved, when the average over 100 episodes of those **scores** is at least +0.5.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = brain.vector_observation_space_size * brain.num_stacked_vector_observations\n",
    "# ###################\n",
    "\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_env(actions):\n",
    "    \n",
    "    env_info = env.step(actions)[brain_name]    # sends the action of each agent for the environment\n",
    "    next_states = env_info.vector_observations  # get the next state for each agent\n",
    "    rewards =  env_info.rewards                 # get the reward for each agent\n",
    "    dones = env_info.local_done                 # check if any episode has finished\n",
    "    \n",
    "    return next_states, rewards, dones, env_info\n",
    "\n",
    "def maddpg(agent, n_episodes=2000, max_t=1000):\n",
    "    \n",
    "    scores = []                        # list with the scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 10 scores\n",
    "    scores_window_mean =  []           # the mean of the last 10 episodes\n",
    "    scores_window_std  =  []           # std of the last 10 episodes\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        agent.reset()\n",
    "        states = env_info.vector_observations             # get the current state for each agent\n",
    "        score = np.zeros(len(agent))                      # initialize the score for each agent\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)                     # selects an action\n",
    "            next_states, rewards, dones,_ = step_env(actions) # sends the action and obtain next_states, rewards, \n",
    "                                                            # dones for each agent\n",
    "            \n",
    "            agent.step(states, actions, rewards, next_states, dones) # execute the action\n",
    "            \n",
    "            states = next_states    # update the current state\n",
    "            score += rewards        # update the score\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break\n",
    "            \n",
    "        scores.append(np.max(score))                           # save most recent score\n",
    "        scores_window.append(np.max(score))                    \n",
    "        w_score_mean = np.mean(scores_window)\n",
    "        scores_window_mean.append(w_score_mean)\n",
    "        scores_window_std.append(np.std(scores_window))\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f} \\tScore: {:.2f}'.format(i_episode, w_score_mean, np.max(score)), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f} \\tScore: {:.2f}'.format(i_episode, w_score_mean, np.max(score)))     \n",
    "        \n",
    "        if w_score_mean >= .5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, w_score_mean))\n",
    "            \n",
    "            filename = \"checkpoints/{}_checkpoint_{}_{}.pth\"\n",
    "\n",
    "            for id in np.arange(len(agent)):\n",
    "                torch.save(agent[id].actor_local.state_dict(),  filename.format(type(agent).__name__, \"actor\", id))\n",
    "                torch.save(agent[id].critic_local.state_dict(), filename.format(type(agent).__name__, \"critic\", id))\n",
    "            break\n",
    "            \n",
    "    return (scores, scores_window_mean, scores_window_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MADDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ddpg.multiagent import MultiAgent\n",
    "agent = MultiAgent(num_agents, state_size, action_size, buffer_size=int(1e5), seed=42, update_frequency=2, n_learns=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the architecture of actor and critic networks of the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in np.arange(len(agent)):\n",
    "    print(\"agent:\", id + 1)\n",
    "    print(agent[id].actor_local)\n",
    "    print(agent[id].critic_local)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training multiple DDPG agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, scores_window_mean, scores_window_std = maddpg(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores, rolling_mean, rolling_std, title, filename=''):\n",
    "    \n",
    "    \"\"\"Plot the scores with the moving average\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.axhline(y=30., xmin=0.0, xmax=1.0, color='r', linestyle='--', alpha=0.9, label=\"Score threshold baseline\")\n",
    "    \n",
    "    plt.plot(np.arange(len(scores)), scores);\n",
    "    plt.plot(np.arange(len(rolling_mean)), rolling_mean,label='Moving average');\n",
    "    plt.fill_between(np.arange(len(rolling_mean)), rolling_mean + rolling_std, rolling_mean - rolling_std, \n",
    "                     facecolor='green', alpha=0.4, label=\"Std of the moving average\");\n",
    "    plt.ylabel(\"Score\"); plt.xlabel(\"Episode #\"); plt.title(title);\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_xlim(0, len(scores) + 1)\n",
    "    \n",
    "    if filename != '':\n",
    "        fig.savefig(filename, format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(scores, np.array(scores_window_mean), np.array(scores_window_std), \n",
    "            title='DDPG Learning Curve', filename='report/figures/{}_score.pdf'.format(type(agent).__name__));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Watch a smart agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoints/{}_checkpoint_actor.pth'.format(type(agent).__name__)))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint/{}_checkpoint_critic.pth'.format(type(agent).__name__)))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state for each agent\n",
    "scores = np.zeros(num_agents)                          # initialize the score for each agent\n",
    "\n",
    "while True:\n",
    "    actions = agent.act(states)                        # select actions from loaded model agent\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "    next_states = env_info.vector_observations         # get the next state for each agent\n",
    "    rewards = env_info.rewards                         # get the reward for each agent\n",
    "    dones = env_info.local_done                        # see if episode has finished\n",
    "    scores += env_info.rewards                         # update the score for each agent\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}